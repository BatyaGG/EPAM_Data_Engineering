{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Spark ETL Job - Restaurant Weather Data Processing\n",
        "\n",
        "This notebook implements a Spark ETL job that:\n",
        "1. Reads restaurant data from CSV files\n",
        "2. Checks for null/invalid latitude and longitude values\n",
        "3. Geocodes missing coordinates using OpenCage Geocoding API\n",
        "4. Generates 4-character geohashes for spatial matching\n",
        "5. Left-joins restaurant data with weather data using geohash\n",
        "6. Stores enriched data in Parquet format with partitioning\n",
        "\n",
        "## Prerequisites\n",
        "- Apache Spark installed locally\n",
        "- Python packages: pyspark, python-geohash, requests, pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages if not already installed\n",
        "!pip install pyspark python-geohash requests pandas pyarrow -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All imports successful!\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import builtins  # For Python's built-in min/max functions\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "# Spark imports\n",
        "from pyspark.sql import SparkSession, DataFrame\n",
        "from pyspark.sql.functions import (\n",
        "    col, udf, when, broadcast, first, coalesce, lit, count, avg,\n",
        "    min as spark_min, max as spark_max  # Rename to avoid conflict with Python builtins\n",
        ")\n",
        "from pyspark.sql.types import (\n",
        "    StringType, DoubleType, IntegerType, StructType, StructField\n",
        ")\n",
        "\n",
        "# Geohash library\n",
        "import geohash as gh\n",
        "\n",
        "# HTTP requests for geocoding\n",
        "import requests\n",
        "\n",
        "print(\"All imports successful!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base path: /Users/batyagg/Projects/EPAM_Data_Engineering/Spark_Task\n",
            "Restaurant data: /Users/batyagg/Projects/EPAM_Data_Engineering/Spark_Task/restaurant_csv\n",
            "Weather data: /Users/batyagg/Projects/EPAM_Data_Engineering/Spark_Task/weather_data/weather\n",
            "Output path: /Users/batyagg/Projects/EPAM_Data_Engineering/Spark_Task/output/enriched_data\n",
            "OpenCage API Key: Set\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "BASE_PATH = os.getcwd()  # Current directory (Spark_Task folder)\n",
        "RESTAURANT_DATA_PATH = os.path.join(BASE_PATH, \"restaurant_csv\")\n",
        "WEATHER_DATA_PATH = os.path.join(BASE_PATH, \"weather_data\", \"weather\")  # Extracted weather parquet data\n",
        "OUTPUT_PATH = os.path.join(BASE_PATH, \"output\", \"enriched_data\")\n",
        "\n",
        "# Geohash precision (4 characters as per task requirements)\n",
        "GEOHASH_PRECISION = 4\n",
        "\n",
        "# OpenCage API Key (set your key here or as environment variable)\n",
        "OPENCAGE_API_KEY = os.environ.get(\"OPENCAGE_API_KEY\", 'cdbec992e2f64810a392f4cf2c6f2aa9')\n",
        "\n",
        "print(f\"Base path: {BASE_PATH}\")\n",
        "print(f\"Restaurant data: {RESTAURANT_DATA_PATH}\")\n",
        "print(f\"Weather data: {WEATHER_DATA_PATH}\")\n",
        "print(f\"Output path: {OUTPUT_PATH}\")\n",
        "print(f\"OpenCage API Key: {'Set' if OPENCAGE_API_KEY else 'Not set (geocoding will be skipped)'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Initialize Spark Session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Using incubator modules: jdk.incubator.vector\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/12/22 23:38:00 WARN Utils: Your hostname, Batyrkhans-MacBook-Pro-4.local, resolves to a loopback address: 127.0.0.1; using 172.20.10.2 instead (on interface en0)\n",
            "25/12/22 23:38:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "25/12/22 23:38:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark version: 4.0.1\n",
            "Spark UI: http://172.20.10.2:4040\n"
          ]
        }
      ],
      "source": [
        "# Create Spark Session\n",
        "spark = (\n",
        "    SparkSession.builder\n",
        "    .appName(\"Restaurant_Weather_ETL\")\n",
        "    .master(\"local[*]\")\n",
        "    .config(\"spark.sql.parquet.compression.codec\", \"snappy\")\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"200\")\n",
        "    .config(\"spark.driver.memory\", \"4g\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "# Set log level to reduce verbosity\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "print(f\"Spark version: {spark.version}\")\n",
        "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Define Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Geohash Generation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Geohash examples (4-character precision):\n",
            "  New York (40.7128, -74.0060): dr5r\n",
            "  Paris (48.8566, 2.3522): u09t\n",
            "  London (51.5074, -0.1278): gcpv\n",
            "  Milan (45.4642, 9.1900): u0nd\n",
            "  Null coordinates: None\n",
            "  Invalid latitude (91, 0): None\n"
          ]
        }
      ],
      "source": [
        "def generate_geohash(lat: Optional[float], lng: Optional[float], precision: int = 4) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Generate a geohash from latitude and longitude coordinates.\n",
        "    \n",
        "    Args:\n",
        "        lat: Latitude coordinate\n",
        "        lng: Longitude coordinate  \n",
        "        precision: Geohash precision (default 4 characters)\n",
        "        \n",
        "    Returns:\n",
        "        Geohash string of specified precision, or None if coordinates are invalid\n",
        "    \"\"\"\n",
        "    if lat is None or lng is None:\n",
        "        return None\n",
        "    \n",
        "    try:\n",
        "        # Validate coordinate ranges\n",
        "        if not (-90 <= lat <= 90) or not (-180 <= lng <= 180):\n",
        "            return None\n",
        "        \n",
        "        # Clamp to open interval to avoid boundary errors\n",
        "        # Using builtins.min/max to avoid conflict with PySpark functions\n",
        "        eps = 1e-9\n",
        "        safe_lat = builtins.min(builtins.max(lat, -90 + eps), 90 - eps)\n",
        "        safe_lng = builtins.min(builtins.max(lng, -180 + eps), 180 - eps)\n",
        "        \n",
        "        return gh.encode(safe_lat, safe_lng, precision=precision)\n",
        "    except (ValueError, TypeError):\n",
        "        return None\n",
        "\n",
        "# Test the function\n",
        "print(\"Geohash examples (4-character precision):\")\n",
        "print(f\"  New York (40.7128, -74.0060): {generate_geohash(40.7128, -74.0060)}\")\n",
        "print(f\"  Paris (48.8566, 2.3522): {generate_geohash(48.8566, 2.3522)}\")\n",
        "print(f\"  London (51.5074, -0.1278): {generate_geohash(51.5074, -0.1278)}\")\n",
        "print(f\"  Milan (45.4642, 9.1900): {generate_geohash(45.4642, 9.1900)}\")\n",
        "print(f\"  Null coordinates: {generate_geohash(None, None)}\")\n",
        "print(f\"  Invalid latitude (91, 0): {generate_geohash(91, 0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 OpenCage Geocoding Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Geocoder initialized\n"
          ]
        }
      ],
      "source": [
        "class OpenCageGeocoder:\n",
        "    \"\"\"OpenCage Geocoding API client for resolving coordinates from addresses.\"\"\"\n",
        "    \n",
        "    BASE_URL = \"https://api.opencagedata.com/geocode/v1/json\"\n",
        "    \n",
        "    def __init__(self, api_key: Optional[str] = None):\n",
        "        self.api_key = api_key\n",
        "        self._last_request_time = 0\n",
        "    \n",
        "    def _rate_limit(self):\n",
        "        \"\"\"Apply rate limiting (1 request per second for free tier).\"\"\"\n",
        "        current_time = time.time()\n",
        "        time_since_last = current_time - self._last_request_time\n",
        "        if time_since_last < 1.0:\n",
        "            time.sleep(1.0 - time_since_last)\n",
        "        self._last_request_time = time.time()\n",
        "    \n",
        "    def geocode(self, city: str, country: str) -> Tuple[Optional[float], Optional[float]]:\n",
        "        \"\"\"\n",
        "        Geocode a city and country to get latitude and longitude.\n",
        "        \n",
        "        Returns:\n",
        "            Tuple of (latitude, longitude) or (None, None) if geocoding fails\n",
        "        \"\"\"\n",
        "        if not self.api_key:\n",
        "            return (None, None)\n",
        "        \n",
        "        self._rate_limit()\n",
        "        query = f\"{city}, {country}\"\n",
        "        \n",
        "        params = {\n",
        "            \"q\": query,\n",
        "            \"key\": self.api_key,\n",
        "            \"limit\": 1,\n",
        "            \"no_annotations\": 1\n",
        "        }\n",
        "        \n",
        "        try:\n",
        "            response = requests.get(self.BASE_URL, params=params, timeout=10)\n",
        "            response.raise_for_status()\n",
        "            \n",
        "            data = response.json()\n",
        "            \n",
        "            if data.get(\"results\") and len(data[\"results\"]) > 0:\n",
        "                geometry = data[\"results\"][0].get(\"geometry\", {})\n",
        "                return (geometry.get(\"lat\"), geometry.get(\"lng\"))\n",
        "            \n",
        "            return (None, None)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Geocoding error for {query}: {e}\")\n",
        "            return (None, None)\n",
        "\n",
        "# Initialize geocoder\n",
        "geocoder = OpenCageGeocoder(OPENCAGE_API_KEY)\n",
        "print(\"Geocoder initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Register Spark UDF for Geohash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Geohash UDF registered successfully\n"
          ]
        }
      ],
      "source": [
        "# Register geohash UDF for use in Spark DataFrames\n",
        "# Note: We import builtins inside the UDF to avoid serialization issues with PySpark min/max\n",
        "\n",
        "@udf(StringType())\n",
        "def geohash_udf(lat, lng):\n",
        "    \"\"\"Spark UDF for generating 4-character geohash.\"\"\"\n",
        "    import builtins as bi  # Import inside UDF to get Python's built-in min/max\n",
        "    import geohash as gh_local  # Re-import inside UDF for serialization\n",
        "    \n",
        "    if lat is None or lng is None:\n",
        "        return None\n",
        "    try:\n",
        "        if not (-90 <= lat <= 90) or not (-180 <= lng <= 180):\n",
        "            return None\n",
        "        eps = 1e-9\n",
        "        safe_lat = bi.min(bi.max(lat, -90 + eps), 90 - eps)\n",
        "        safe_lng = bi.min(bi.max(lng, -180 + eps), 180 - eps)\n",
        "        return gh_local.encode(safe_lat, safe_lng, precision=4)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "print(\"Geohash UDF registered successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Read Restaurant Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total restaurant records: 1997\n",
            "\n",
            "Schema:\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- franchise_id: integer (nullable = true)\n",
            " |-- franchise_name: string (nullable = true)\n",
            " |-- restaurant_franchise_id: integer (nullable = true)\n",
            " |-- country: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- lat: double (nullable = true)\n",
            " |-- lng: double (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Read restaurant data from CSV files\n",
        "restaurant_df = (\n",
        "    spark.read\n",
        "    .option(\"header\", \"true\")\n",
        "    .option(\"inferSchema\", \"true\")\n",
        "    .csv(RESTAURANT_DATA_PATH)\n",
        ")\n",
        "\n",
        "# Ensure correct column types\n",
        "restaurant_df = restaurant_df.select(\n",
        "    col(\"id\").cast(\"long\"),\n",
        "    col(\"franchise_id\").cast(\"int\"),\n",
        "    col(\"franchise_name\").cast(\"string\"),\n",
        "    col(\"restaurant_franchise_id\").cast(\"int\"),\n",
        "    col(\"country\").cast(\"string\"),\n",
        "    col(\"city\").cast(\"string\"),\n",
        "    col(\"lat\").cast(\"double\"),\n",
        "    col(\"lng\").cast(\"double\")\n",
        ")\n",
        "\n",
        "print(f\"Total restaurant records: {restaurant_df.count()}\")\n",
        "print(\"\\nSchema:\")\n",
        "restaurant_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample restaurant data:\n",
            "+------------+------------+--------------------+-----------------------+-------+--------------+------+-------+\n",
            "|id          |franchise_id|franchise_name      |restaurant_franchise_id|country|city          |lat   |lng    |\n",
            "+------------+------------+--------------------+-----------------------+-------+--------------+------+-------+\n",
            "|197568495625|10          |The Golden Spoon    |24784                  |US     |Decatur       |34.578|-87.021|\n",
            "|17179869242 |59          |Azalea Cafe         |10902                  |FR     |Paris         |48.861|2.368  |\n",
            "|214748364826|27          |The Corner Cafe     |92040                  |US     |Rapid City    |44.08 |-103.25|\n",
            "|154618822706|51          |The Pizzeria        |41484                  |AT     |Vienna        |48.213|16.413 |\n",
            "|163208757312|65          |Chef's Corner       |96638                  |GB     |London        |51.495|-0.191 |\n",
            "|68719476763 |28          |The Spicy Pickle    |77517                  |US     |Grayling      |44.657|-84.744|\n",
            "|223338299419|28          |The Spicy Pickle    |36937                  |US     |Oswego        |43.452|-76.532|\n",
            "|240518168650|75          |Greenhouse Cafe     |93164                  |NL     |Amsterdam     |52.37 |4.897  |\n",
            "|128849018936|57          |The Yellow Submarine|5679                   |FR     |Paris         |48.872|2.335  |\n",
            "|197568495635|20          |The Brasserie       |24784                  |US     |Jeffersonville|39.616|-83.612|\n",
            "+------------+------------+--------------------+-----------------------+-------+--------------+------+-------+\n",
            "only showing top 10 rows\n"
          ]
        }
      ],
      "source": [
        "# Display sample restaurant data\n",
        "print(\"Sample restaurant data:\")\n",
        "restaurant_df.show(10, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Coordinate statistics:\n",
            "+-------+-----------------+------------------+\n",
            "|summary|              lat|               lng|\n",
            "+-------+-----------------+------------------+\n",
            "|  count|             1996|              1996|\n",
            "|   mean|43.99021893787576|-34.52430861723447|\n",
            "| stddev|7.142288389673525| 49.89812670573716|\n",
            "|    min|          -25.437|          -159.481|\n",
            "|    max|             58.3|           115.164|\n",
            "+-------+-----------------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Show statistics for coordinates\n",
        "print(\"Coordinate statistics:\")\n",
        "restaurant_df.select(\"lat\", \"lng\").describe().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Check for Null/Invalid Coordinates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Records with NULL coordinates: 1\n",
            "\n",
            "Sample records with NULL coordinates:\n",
            "+-----------+------------+--------------+-----------------------+-------+------+----+----+\n",
            "|id         |franchise_id|franchise_name|restaurant_franchise_id|country|city  |lat |lng |\n",
            "+-----------+------------+--------------+-----------------------+-------+------+----+----+\n",
            "|85899345920|1           |Savoria       |18952                  |US     |Dillon|NULL|NULL|\n",
            "+-----------+------------+--------------+-----------------------+-------+------+----+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Identify records with null coordinates\n",
        "null_coords_df = restaurant_df.filter(\n",
        "    col(\"lat\").isNull() | col(\"lng\").isNull()\n",
        ")\n",
        "\n",
        "null_count = null_coords_df.count()\n",
        "print(f\"Records with NULL coordinates: {null_count}\")\n",
        "\n",
        "if null_count > 0:\n",
        "    print(\"\\nSample records with NULL coordinates:\")\n",
        "    null_coords_df.show(10, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Records with INVALID coordinates (out of range): 0\n"
          ]
        }
      ],
      "source": [
        "# Identify records with invalid coordinates (outside valid range)\n",
        "invalid_coords_df = restaurant_df.filter(\n",
        "    (col(\"lat\").isNotNull() & col(\"lng\").isNotNull()) &\n",
        "    (\n",
        "        (col(\"lat\") < -90) | (col(\"lat\") > 90) |\n",
        "        (col(\"lng\") < -180) | (col(\"lng\") > 180)\n",
        "    )\n",
        ")\n",
        "\n",
        "invalid_count = invalid_coords_df.count()\n",
        "print(f\"Records with INVALID coordinates (out of range): {invalid_count}\")\n",
        "\n",
        "if invalid_count > 0:\n",
        "    print(\"\\nSample records with invalid coordinates:\")\n",
        "    invalid_coords_df.select(\"id\", \"franchise_name\", \"city\", \"country\", \"lat\", \"lng\").show(20, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "DATA QUALITY SUMMARY\n",
            "==================================================\n",
            "Total records:              1,997\n",
            "Valid coordinates:          1,996 (99.9%)\n",
            "NULL coordinates:           1 (0.1%)\n",
            "Invalid coordinates:        0 (0.0%)\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Summary of data quality issues\n",
        "total_records = restaurant_df.count()\n",
        "valid_records = total_records - null_count - invalid_count\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"DATA QUALITY SUMMARY\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Total records:              {total_records:,}\")\n",
        "print(f\"Valid coordinates:          {valid_records:,} ({100*valid_records/total_records:.1f}%)\")\n",
        "print(f\"NULL coordinates:           {null_count:,} ({100*null_count/total_records:.1f}%)\")\n",
        "print(f\"Invalid coordinates:        {invalid_count:,} ({100*invalid_count/total_records:.1f}%)\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Geocode Missing Coordinates\n",
        "\n",
        "This step uses the OpenCage Geocoding API to fill in missing coordinates based on city and country.\n",
        "\n",
        "**Note:** Requires OPENCAGE_API_KEY environment variable to be set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def geocode_missing_coordinates(df: DataFrame, geocoder: OpenCageGeocoder) -> DataFrame:\n",
        "    \"\"\"\n",
        "    Fill in missing coordinates using OpenCage Geocoding API.\n",
        "    \"\"\"\n",
        "    if not geocoder.api_key:\n",
        "        print(\"Warning: No OpenCage API key provided. Skipping geocoding.\")\n",
        "        return df\n",
        "    \n",
        "    # Get records needing geocoding\n",
        "    needs_geocoding = df.filter(col(\"lat\").isNull() | col(\"lng\").isNull())\n",
        "    \n",
        "    if needs_geocoding.count() == 0:\n",
        "        print(\"No records need geocoding.\")\n",
        "        return df\n",
        "    \n",
        "    # Get unique city/country combinations\n",
        "    unique_locations = (\n",
        "        needs_geocoding\n",
        "        .select(\"city\", \"country\")\n",
        "        .distinct()\n",
        "        .collect()\n",
        "    )\n",
        "    \n",
        "    print(f\"Geocoding {len(unique_locations)} unique locations...\")\n",
        "    \n",
        "    # Geocode each unique location\n",
        "    geocoded_data = []\n",
        "    for i, row in enumerate(unique_locations):\n",
        "        lat, lng = geocoder.geocode(row.city, row.country)\n",
        "        geocoded_data.append((row.city, row.country, lat, lng))\n",
        "        if (i + 1) % 10 == 0:\n",
        "            print(f\"  Processed {i + 1}/{len(unique_locations)} locations\")\n",
        "    \n",
        "    # Create lookup DataFrame\n",
        "    geocoded_schema = StructType([\n",
        "        StructField(\"lookup_city\", StringType(), True),\n",
        "        StructField(\"lookup_country\", StringType(), True),\n",
        "        StructField(\"geocoded_lat\", DoubleType(), True),\n",
        "        StructField(\"geocoded_lng\", DoubleType(), True),\n",
        "    ])\n",
        "    \n",
        "    geocoded_df = spark.createDataFrame(geocoded_data, geocoded_schema)\n",
        "    \n",
        "    # Join and coalesce coordinates\n",
        "    result = df.join(\n",
        "        broadcast(geocoded_df),\n",
        "        (df.city == geocoded_df.lookup_city) & (df.country == geocoded_df.lookup_country),\n",
        "        \"left\"\n",
        "    ).select(\n",
        "        df.id,\n",
        "        df.franchise_id,\n",
        "        df.franchise_name,\n",
        "        df.restaurant_franchise_id,\n",
        "        df.country,\n",
        "        df.city,\n",
        "        coalesce(df.lat, geocoded_df.geocoded_lat).alias(\"lat\"),\n",
        "        coalesce(df.lng, geocoded_df.geocoded_lng).alias(\"lng\")\n",
        "    )\n",
        "    \n",
        "    print(\"Geocoding complete!\")\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Geocoding missing coordinates...\n",
            "Geocoding 1 unique locations...\n",
            "Geocoding complete!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Remaining records with null coordinates: 0\n"
          ]
        }
      ],
      "source": [
        "# Apply geocoding if API key is available\n",
        "if OPENCAGE_API_KEY and null_count > 0:\n",
        "    print(\"Geocoding missing coordinates...\")\n",
        "    restaurant_df = geocode_missing_coordinates(restaurant_df, geocoder)\n",
        "    \n",
        "    # Check remaining null coordinates\n",
        "    remaining_nulls = restaurant_df.filter(col(\"lat\").isNull() | col(\"lng\").isNull()).count()\n",
        "    print(f\"Remaining records with null coordinates: {remaining_nulls}\")\n",
        "else:\n",
        "    if not OPENCAGE_API_KEY:\n",
        "        print(\"Skipping geocoding - no API key provided\")\n",
        "        print(\"To enable geocoding, set: export OPENCAGE_API_KEY=your_key\")\n",
        "    else:\n",
        "        print(\"No null coordinates to geocode\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Generate Geohash Column\n",
        "\n",
        "Generate a 4-character geohash for each restaurant based on its coordinates.\n",
        "\n",
        "**Why 4 characters?**\n",
        "- 4-character geohashes represent areas of approximately 39km x 20km\n",
        "- This provides good granularity for matching restaurants with nearby weather stations\n",
        "- Balances precision with join efficiency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added geohash column to restaurant data\n",
            "\n",
            "Sample data with geohash:\n",
            "+------------+--------------------+--------------+-------+------+-------+-------+\n",
            "|id          |franchise_name      |city          |country|lat   |lng    |geohash|\n",
            "+------------+--------------------+--------------+-------+------+-------+-------+\n",
            "|197568495625|The Golden Spoon    |Decatur       |US     |34.578|-87.021|dn4h   |\n",
            "|17179869242 |Azalea Cafe         |Paris         |FR     |48.861|2.368  |u09t   |\n",
            "|214748364826|The Corner Cafe     |Rapid City    |US     |44.08 |-103.25|9xyd   |\n",
            "|154618822706|The Pizzeria        |Vienna        |AT     |48.213|16.413 |u2ed   |\n",
            "|163208757312|Chef's Corner       |London        |GB     |51.495|-0.191 |gcpu   |\n",
            "|68719476763 |The Spicy Pickle    |Grayling      |US     |44.657|-84.744|dpgw   |\n",
            "|223338299419|The Spicy Pickle    |Oswego        |US     |43.452|-76.532|dr9x   |\n",
            "|240518168650|Greenhouse Cafe     |Amsterdam     |NL     |52.37 |4.897  |u173   |\n",
            "|128849018936|The Yellow Submarine|Paris         |FR     |48.872|2.335  |u09w   |\n",
            "|197568495635|The Brasserie       |Jeffersonville|US     |39.616|-83.612|dph9   |\n",
            "|68719476768 |The Blue Elephant   |Milan         |IT     |45.479|9.146  |u0nd   |\n",
            "|51539607582 |Bistro 42           |Milan         |IT     |45.444|9.153  |u0nd   |\n",
            "|94489280554 |The Food House      |Paris         |FR     |48.867|2.329  |u09t   |\n",
            "|206158430215|The Green Olive     |Haltom City   |US     |32.789|-97.28 |9vff   |\n",
            "|154618822657|Bella Cucina        |Fort Pierce   |US     |27.412|-80.391|dhyg   |\n",
            "+------------+--------------------+--------------+-------+------+-------+-------+\n",
            "only showing top 15 rows\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Add geohash column to restaurant data\n",
        "restaurant_with_geohash = restaurant_df.withColumn(\n",
        "    \"geohash\",\n",
        "    geohash_udf(col(\"lat\"), col(\"lng\"))\n",
        ")\n",
        "\n",
        "print(\"Added geohash column to restaurant data\")\n",
        "print(\"\\nSample data with geohash:\")\n",
        "restaurant_with_geohash.select(\n",
        "    \"id\", \"franchise_name\", \"city\", \"country\", \"lat\", \"lng\", \"geohash\"\n",
        ").show(15, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Geohash generation statistics:\n",
            "+-------------+--------------------+-----------------------+\n",
            "|total_records|records_with_geohash|records_without_geohash|\n",
            "+-------------+--------------------+-----------------------+\n",
            "|         1997|                1997|                      0|\n",
            "+-------------+--------------------+-----------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Verify geohash generation\n",
        "geohash_stats = restaurant_with_geohash.agg(\n",
        "    count(\"*\").alias(\"total_records\"),\n",
        "    count(\"geohash\").alias(\"records_with_geohash\"),\n",
        "    (count(\"*\") - count(\"geohash\")).alias(\"records_without_geohash\")\n",
        ")\n",
        "\n",
        "print(\"Geohash generation statistics:\")\n",
        "geohash_stats.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique geohashes in restaurant data: 692\n",
            "\n",
            "Top 10 geohashes by restaurant count:\n",
            "+-------+-----+\n",
            "|geohash|count|\n",
            "+-------+-----+\n",
            "|   gcpv|  208|\n",
            "|   u09t|  182|\n",
            "|   sp3e|  171|\n",
            "|   u09w|  169|\n",
            "|   u0nd|  122|\n",
            "|   u2ed|  116|\n",
            "|   gcpu|  101|\n",
            "|   u173|   61|\n",
            "|   u179|    8|\n",
            "|   u0n6|    8|\n",
            "+-------+-----+\n",
            "only showing top 10 rows\n"
          ]
        }
      ],
      "source": [
        "# Show unique geohash count\n",
        "unique_geohashes = restaurant_with_geohash.select(\"geohash\").distinct().count()\n",
        "print(f\"Unique geohashes in restaurant data: {unique_geohashes}\")\n",
        "\n",
        "# Show top geohashes by count\n",
        "print(\"\\nTop 10 geohashes by restaurant count:\")\n",
        "restaurant_with_geohash.groupBy(\"geohash\").count().orderBy(col(\"count\").desc()).show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Read Weather Data\n",
        "\n",
        "Read the weather data from the extracted parquet files.\n",
        "\n",
        "**Weather Data Schema:**\n",
        "- `lng`: Longitude\n",
        "- `lat`: Latitude\n",
        "- `avg_tmpr_f`: Average temperature in Fahrenheit\n",
        "- `avg_tmpr_c`: Average temperature in Celsius\n",
        "- `wthr_date`: Weather observation date\n",
        "- `year`, `month`, `day`: Partition columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading weather data from: /Users/batyagg/Projects/EPAM_Data_Engineering/Spark_Task/weather_data/weather\n",
            "\n",
            "Total weather records: 112,394,743\n",
            "\n",
            "Weather data schema:\n",
            "root\n",
            " |-- lng: double (nullable = true)\n",
            " |-- lat: double (nullable = true)\n",
            " |-- avg_tmpr_f: double (nullable = true)\n",
            " |-- avg_tmpr_c: double (nullable = true)\n",
            " |-- wthr_date: string (nullable = true)\n",
            " |-- year: integer (nullable = true)\n",
            " |-- month: integer (nullable = true)\n",
            " |-- day: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Read weather data from parquet files\n",
        "print(f\"Reading weather data from: {WEATHER_DATA_PATH}\")\n",
        "\n",
        "weather_df = spark.read.parquet(WEATHER_DATA_PATH)\n",
        "\n",
        "print(f\"\\nTotal weather records: {weather_df.count():,}\")\n",
        "print(\"\\nWeather data schema:\")\n",
        "weather_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample weather data:\n",
            "+--------+-------+----------+----------+----------+----+-----+---+\n",
            "|lng     |lat    |avg_tmpr_f|avg_tmpr_c|wthr_date |year|month|day|\n",
            "+--------+-------+----------+----------+----------+----+-----+---+\n",
            "|-111.09 |18.6251|80.7      |27.1      |2017-08-29|2017|8    |29 |\n",
            "|-111.042|18.6305|80.7      |27.1      |2017-08-29|2017|8    |29 |\n",
            "|-110.995|18.6358|80.7      |27.1      |2017-08-29|2017|8    |29 |\n",
            "|-110.947|18.6412|80.9      |27.2      |2017-08-29|2017|8    |29 |\n",
            "|-110.9  |18.6465|80.9      |27.2      |2017-08-29|2017|8    |29 |\n",
            "|-110.852|18.6518|80.9      |27.2      |2017-08-29|2017|8    |29 |\n",
            "|-110.804|18.6571|80.9      |27.2      |2017-08-29|2017|8    |29 |\n",
            "|-105.068|19.1765|82.4      |28.0      |2017-08-29|2017|8    |29 |\n",
            "|-105.02 |19.1799|82.0      |27.8      |2017-08-29|2017|8    |29 |\n",
            "|-104.972|19.1832|82.0      |27.8      |2017-08-29|2017|8    |29 |\n",
            "+--------+-------+----------+----------+----------+----+-----+---+\n",
            "only showing top 10 rows\n"
          ]
        }
      ],
      "source": [
        "# Display sample weather data\n",
        "print(\"Sample weather data:\")\n",
        "weather_df.show(10, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Weather data statistics:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/12/22 23:38:12 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
            "[Stage 52:===================================>                    (10 + 6) / 16]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+------------------+------------------+------------------+-----------------+\n",
            "|summary|               lat|               lng|        avg_tmpr_c|       avg_tmpr_f|\n",
            "+-------+------------------+------------------+------------------+-----------------+\n",
            "|  count|         112394743|         112394743|         112394743|        112394743|\n",
            "|   mean|  36.0821450186546|-39.06072982875904|15.324629014899868|59.58432871365851|\n",
            "| stddev|26.116940502760713| 88.74779671660777|10.409133429838565|18.73636861533826|\n",
            "|    min|          -59.7956|          -179.796|             -43.3|            -46.0|\n",
            "|    max|           85.5535|             180.0|              44.2|            111.5|\n",
            "+-------+------------------+------------------+------------------+-----------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Weather data statistics\n",
        "print(\"Weather data statistics:\")\n",
        "weather_df.select(\"lat\", \"lng\", \"avg_tmpr_c\", \"avg_tmpr_f\").describe().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Weather data date range:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 55:=================================================>      (14 + 2) / 16]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+----------+\n",
            "|  min_date|  max_date|\n",
            "+----------+----------+\n",
            "|2016-10-01|2017-09-30|\n",
            "+----------+----------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Check date range in weather data\n",
        "print(\"Weather data date range:\")\n",
        "weather_df.select(\n",
        "    spark_min(\"wthr_date\").alias(\"min_date\"),\n",
        "    spark_max(\"wthr_date\").alias(\"max_date\")\n",
        ").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.1 Add Geohash to Weather Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adding geohash column to weather data...\n",
            "Sample weather data with geohash:\n",
            "+-------+--------+-------+----------+----------+\n",
            "|    lat|     lng|geohash|avg_tmpr_c| wthr_date|\n",
            "+-------+--------+-------+----------+----------+\n",
            "|18.6251| -111.09|   9e31|      27.1|2017-08-29|\n",
            "|18.6305|-111.042|   9e31|      27.1|2017-08-29|\n",
            "|18.6358|-110.995|   9e34|      27.1|2017-08-29|\n",
            "|18.6412|-110.947|   9e34|      27.2|2017-08-29|\n",
            "|18.6465|  -110.9|   9e34|      27.2|2017-08-29|\n",
            "|18.6518|-110.852|   9e34|      27.2|2017-08-29|\n",
            "|18.6571|-110.804|   9e34|      27.2|2017-08-29|\n",
            "|19.1765|-105.068|   9emm|      28.0|2017-08-29|\n",
            "|19.1799| -105.02|   9emm|      27.8|2017-08-29|\n",
            "|19.1832|-104.972|   9emm|      27.8|2017-08-29|\n",
            "+-------+--------+-------+----------+----------+\n",
            "only showing top 10 rows\n"
          ]
        }
      ],
      "source": [
        "# Add geohash column to weather data\n",
        "print(\"Adding geohash column to weather data...\")\n",
        "\n",
        "weather_with_geohash = weather_df.withColumn(\n",
        "    \"geohash\",\n",
        "    geohash_udf(col(\"lat\"), col(\"lng\"))\n",
        ")\n",
        "\n",
        "print(\"Sample weather data with geohash:\")\n",
        "weather_with_geohash.select(\"lat\", \"lng\", \"geohash\", \"avg_tmpr_c\", \"wthr_date\").show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 59:====================================================>   (15 + 1) / 16]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique geohashes in weather data: 346,655\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Count unique geohashes in weather data\n",
        "weather_unique_geohashes = weather_with_geohash.select(\"geohash\").distinct().count()\n",
        "print(f\"Unique geohashes in weather data: {weather_unique_geohashes:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Left Join Restaurant and Weather Data\n",
        "\n",
        "Join the restaurant data with weather data using the 4-character geohash.\n",
        "\n",
        "**Important considerations:**\n",
        "- Use LEFT JOIN to keep all restaurant records\n",
        "- Deduplicate weather data by geohash to avoid data multiplication\n",
        "- Ensure idempotency (same input → same output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deduplicating weather data by geohash...\n",
            "Weather records before deduplication: 112,394,743\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 68:=================================================>      (14 + 2) / 16]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Weather records after deduplication (unique geohashes): 346,655\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Deduplicate weather data by geohash to prevent data multiplication\n",
        "# Aggregate weather metrics by geohash (average values across all dates and locations with same geohash)\n",
        "\n",
        "print(\"Deduplicating weather data by geohash...\")\n",
        "\n",
        "weather_dedupe = (\n",
        "    weather_with_geohash\n",
        "    .groupBy(\"geohash\")\n",
        "    .agg(\n",
        "        first(\"lat\", ignorenulls=True).alias(\"weather_lat\"),\n",
        "        first(\"lng\", ignorenulls=True).alias(\"weather_lng\"),\n",
        "        avg(\"avg_tmpr_f\").alias(\"weather_avg_tmpr_f\"),\n",
        "        avg(\"avg_tmpr_c\").alias(\"weather_avg_tmpr_c\"),\n",
        "        first(\"wthr_date\", ignorenulls=True).alias(\"weather_wthr_date\")\n",
        "    )\n",
        ")\n",
        "\n",
        "# Rename geohash to avoid collision\n",
        "weather_dedupe = weather_dedupe.withColumnRenamed(\"geohash\", \"weather_geohash\")\n",
        "\n",
        "print(f\"Weather records before deduplication: {weather_with_geohash.count():,}\")\n",
        "print(f\"Weather records after deduplication (unique geohashes): {weather_dedupe.count():,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 78:====================================================>   (15 + 1) / 16]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique geohashes in restaurants: 692\n",
            "Unique geohashes in weather: 346655\n",
            "Matching geohashes: 685\n",
            "\n",
            "Sample matching geohashes: ['9ysv', '9q96', 'c2kx', '9wst', 'w4ru', 'djbb', '9zpp', 'dpdg', '87zc', '9vfg']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Check for matching geohashes between restaurant and weather data\n",
        "restaurant_geohashes = set(restaurant_with_geohash.select(\"geohash\").distinct().rdd.flatMap(lambda x: x).collect())\n",
        "weather_geohashes = set(weather_dedupe.select(\"weather_geohash\").distinct().rdd.flatMap(lambda x: x).collect())\n",
        "\n",
        "matching_geohashes = restaurant_geohashes.intersection(weather_geohashes)\n",
        "\n",
        "print(f\"Unique geohashes in restaurants: {len(restaurant_geohashes)}\")\n",
        "print(f\"Unique geohashes in weather: {len(weather_geohashes)}\")\n",
        "print(f\"Matching geohashes: {len(matching_geohashes)}\")\n",
        "print(f\"\\nSample matching geohashes: {list(matching_geohashes)[:10]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performing left join on geohash...\n",
            "\n",
            "Records before join: 1997\n",
            "Records after join: 1997\n",
            "\n",
            "✓ Record count preserved (no data multiplication)\n"
          ]
        }
      ],
      "source": [
        "# Perform LEFT JOIN\n",
        "print(\"Performing left join on geohash...\")\n",
        "\n",
        "enriched_df = restaurant_with_geohash.join(\n",
        "    weather_dedupe,\n",
        "    restaurant_with_geohash.geohash == weather_dedupe.weather_geohash,\n",
        "    \"left\"\n",
        ").drop(\"weather_geohash\")\n",
        "\n",
        "print(f\"\\nRecords before join: {restaurant_with_geohash.count()}\")\n",
        "print(f\"Records after join: {enriched_df.count()}\")\n",
        "print(\"\\n✓ Record count preserved (no data multiplication)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample enriched data (restaurant + weather):\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+---------------------+----------+-------+-------+------------------+------------------+-----------------+\n",
            "|id          |franchise_name       |city      |country|geohash|weather_avg_tmpr_c|weather_avg_tmpr_f|weather_wthr_date|\n",
            "+------------+---------------------+----------+-------+-------+------------------+------------------+-----------------+\n",
            "|120259084300|The Firehouse        |Branson   |US     |9yt8   |21.164558232931736|70.09377510040161 |2017-08-29       |\n",
            "|103079215121|The Harvest Room     |Mount Airy|US     |dnqx   |19.375336322869963|66.87460252751735 |2017-08-29       |\n",
            "|197568495640|The Cozy Cafe        |Oskaloosa |US     |9zq5   |18.437726449275356|65.18727355072461 |2017-09-07       |\n",
            "|197568495625|The Golden Spoon     |Decatur   |US     |dn4h   |22.17667224080268 |71.91659698996655 |2017-09-07       |\n",
            "|171798691904|Chef's Corner        |London    |GB     |gcpv   |13.345652173913043|56.02065217391305 |2017-08-13       |\n",
            "|171798691900|Blue Water Bistro    |London    |GB     |gcpv   |13.345652173913043|56.02065217391305 |2017-08-13       |\n",
            "|266287972429|The Hot Spot         |London    |GB     |gcpv   |13.345652173913043|56.02065217391305 |2017-08-13       |\n",
            "|257698037828|Dragonfly Cafe       |Paddington|GB     |gcpv   |13.345652173913043|56.02065217391305 |2017-08-13       |\n",
            "|154618822732|Harvest Kitchen      |London    |GB     |gcpv   |13.345652173913043|56.02065217391305 |2017-08-13       |\n",
            "|23          |The Fisherman's Catch|Barcelona |ES     |sp3e   |21.233152173913044|70.22282608695652 |2017-08-15       |\n",
            "|51539607572 |The Lazy Daisy       |Barcelona |ES     |sp3e   |21.233152173913044|70.22282608695652 |2017-08-15       |\n",
            "|154618822678|The Hungry Pig       |Barcelona |ES     |sp3e   |21.233152173913044|70.22282608695652 |2017-08-15       |\n",
            "|231928234004|The Lazy Daisy       |Barcelona |ES     |sp3e   |21.233152173913044|70.22282608695652 |2017-08-15       |\n",
            "|223338299411|The Brasserie        |Barcelona |ES     |sp3e   |21.233152173913044|70.22282608695652 |2017-08-15       |\n",
            "|206158430236|The Olive Branch     |Barcelona |ES     |sp3e   |21.233152173913044|70.22282608695652 |2017-08-15       |\n",
            "+------------+---------------------+----------+-------+-------+------------------+------------------+-----------------+\n",
            "only showing top 15 rows\n"
          ]
        }
      ],
      "source": [
        "# Display sample enriched data\n",
        "print(\"Sample enriched data (restaurant + weather):\")\n",
        "enriched_df.select(\n",
        "    \"id\", \"franchise_name\", \"city\", \"country\", \"geohash\",\n",
        "    \"weather_avg_tmpr_c\", \"weather_avg_tmpr_f\", \"weather_wthr_date\"\n",
        ").show(15, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "JOIN STATISTICS\n",
            "==================================================\n",
            "Total records:        1,997\n",
            "Matched with weather: 1,875 (93.9%)\n",
            "No weather match:     122 (6.1%)\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Check join statistics\n",
        "matched = enriched_df.filter(col(\"weather_avg_tmpr_c\").isNotNull()).count()\n",
        "unmatched = enriched_df.filter(col(\"weather_avg_tmpr_c\").isNull()).count()\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"JOIN STATISTICS\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Total records:        {enriched_df.count():,}\")\n",
        "print(f\"Matched with weather: {matched:,} ({100*matched/enriched_df.count():.1f}%)\")\n",
        "print(f\"No weather match:     {unmatched:,} ({100*unmatched/enriched_df.count():.1f}%)\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Final enriched data schema:\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- franchise_id: integer (nullable = true)\n",
            " |-- franchise_name: string (nullable = true)\n",
            " |-- restaurant_franchise_id: integer (nullable = true)\n",
            " |-- country: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- lat: double (nullable = true)\n",
            " |-- lng: double (nullable = true)\n",
            " |-- geohash: string (nullable = true)\n",
            " |-- weather_lat: double (nullable = true)\n",
            " |-- weather_lng: double (nullable = true)\n",
            " |-- weather_avg_tmpr_f: double (nullable = true)\n",
            " |-- weather_avg_tmpr_c: double (nullable = true)\n",
            " |-- weather_wthr_date: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Show final schema\n",
        "print(\"\\nFinal enriched data schema:\")\n",
        "enriched_df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Save Enriched Data to Parquet\n",
        "\n",
        "Store the enriched data in Parquet format with partitioning by country."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing enriched data to: /Users/batyagg/Projects/EPAM_Data_Engineering/Spark_Task/output/enriched_data\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 135:>                                                      (0 + 10) / 11]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✓ Data written successfully!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Write enriched data to Parquet with partitioning\n",
        "print(f\"Writing enriched data to: {OUTPUT_PATH}\")\n",
        "\n",
        "(\n",
        "    enriched_df\n",
        "    .write\n",
        "    .mode(\"overwrite\")\n",
        "    .partitionBy(\"country\")\n",
        "    .parquet(OUTPUT_PATH)\n",
        ")\n",
        "\n",
        "print(\"\\n✓ Data written successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output directory structure:\n",
            "  ._SUCCESS.crc\n",
            "  _SUCCESS\n",
            "  country=AT/ (2 parquet files)\n",
            "  country=ES/ (1 parquet files)\n",
            "  country=FR/ (2 parquet files)\n",
            "  country=GB/ (4 parquet files)\n",
            "  country=IT/ (3 parquet files)\n",
            "  country=NL/ (3 parquet files)\n",
            "  country=US/ (11 parquet files)\n"
          ]
        }
      ],
      "source": [
        "# Verify output\n",
        "print(\"Output directory structure:\")\n",
        "for item in sorted(os.listdir(OUTPUT_PATH)):\n",
        "    item_path = os.path.join(OUTPUT_PATH, item)\n",
        "    if os.path.isdir(item_path):\n",
        "        files = len([f for f in os.listdir(item_path) if f.endswith('.parquet')])\n",
        "        print(f\"  {item}/ ({files} parquet files)\")\n",
        "    else:\n",
        "        print(f\"  {item}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Verifying saved data...\n",
            "Records read back: 1997\n",
            "Original records: 1997\n",
            "\n",
            "✓ Data integrity verified: True\n"
          ]
        }
      ],
      "source": [
        "# Read back and verify\n",
        "print(\"\\nVerifying saved data...\")\n",
        "verified_df = spark.read.parquet(OUTPUT_PATH)\n",
        "\n",
        "print(f\"Records read back: {verified_df.count()}\")\n",
        "print(f\"Original records: {enriched_df.count()}\")\n",
        "print(f\"\\n✓ Data integrity verified: {verified_df.count() == enriched_df.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Summary and Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "ETL PIPELINE SUMMARY\n",
            "============================================================\n",
            "\n",
            "INPUT:\n",
            "  Restaurant records:     1,997\n",
            "  Weather records:        112,394,743\n",
            "\n",
            "DATA QUALITY:\n",
            "  Null coordinates:       1\n",
            "  Invalid coordinates:    0\n",
            "\n",
            "GEOHASH:\n",
            "  Precision:              4 characters\n",
            "  Restaurant geohashes:   692\n",
            "  Weather geohashes:      346,655\n",
            "  Matching geohashes:     685\n",
            "\n",
            "JOIN RESULTS:\n",
            "  Total enriched records: 1,997\n",
            "  With weather data:      1,875 (93.9%)\n",
            "  Without weather data:   122\n",
            "\n",
            "OUTPUT:\n",
            "  Format:                 Parquet (Snappy compression)\n",
            "  Partitioning:           By country\n",
            "  Location:               /Users/batyagg/Projects/EPAM_Data_Engineering/Spark_Task/output/enriched_data\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Final summary\n",
        "print(\"=\" * 60)\n",
        "print(\"ETL PIPELINE SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\")\n",
        "print(f\"INPUT:\")\n",
        "print(f\"  Restaurant records:     {restaurant_df.count():,}\")\n",
        "print(f\"  Weather records:        {weather_df.count():,}\")\n",
        "print(f\"\")\n",
        "print(f\"DATA QUALITY:\")\n",
        "print(f\"  Null coordinates:       {null_count:,}\")\n",
        "print(f\"  Invalid coordinates:    {invalid_count:,}\")\n",
        "print(f\"\")\n",
        "print(f\"GEOHASH:\")\n",
        "print(f\"  Precision:              {GEOHASH_PRECISION} characters\")\n",
        "print(f\"  Restaurant geohashes:   {unique_geohashes:,}\")\n",
        "print(f\"  Weather geohashes:      {weather_unique_geohashes:,}\")\n",
        "print(f\"  Matching geohashes:     {len(matching_geohashes):,}\")\n",
        "print(f\"\")\n",
        "print(f\"JOIN RESULTS:\")\n",
        "print(f\"  Total enriched records: {enriched_df.count():,}\")\n",
        "print(f\"  With weather data:      {matched:,} ({100*matched/enriched_df.count():.1f}%)\")\n",
        "print(f\"  Without weather data:   {unmatched:,}\")\n",
        "print(f\"\")\n",
        "print(f\"OUTPUT:\")\n",
        "print(f\"  Format:                 Parquet (Snappy compression)\")\n",
        "print(f\"  Partitioning:           By country\")\n",
        "print(f\"  Location:               {OUTPUT_PATH}\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Records by country:\n",
            "+-------+-----+\n",
            "|country|count|\n",
            "+-------+-----+\n",
            "|     US|  835|\n",
            "|     FR|  347|\n",
            "|     GB|  317|\n",
            "|     ES|  171|\n",
            "|     IT|  130|\n",
            "|     AT|  121|\n",
            "|     NL|   76|\n",
            "+-------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Records per country\n",
        "print(\"\\nRecords by country:\")\n",
        "enriched_df.groupBy(\"country\").count().orderBy(col(\"count\").desc()).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Average temperature by country (matched records only):\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 171:===================================================>   (15 + 1) / 16]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+------------+------------------+\n",
            "|country|record_count| avg_temperature_c|\n",
            "+-------+------------+------------------+\n",
            "|     US|         829|19.258809987070315|\n",
            "|     FR|         347|14.061344129808282|\n",
            "|     GB|         317|13.353674050198899|\n",
            "|     ES|         171|21.233152173913012|\n",
            "|     IT|         130|17.223269230769265|\n",
            "|     NL|          76|14.080499141876443|\n",
            "|     AT|           5|15.106521739130434|\n",
            "+-------+------------+------------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Average temperature by country (for matched records)\n",
        "print(\"\\nAverage temperature by country (matched records only):\")\n",
        "(\n",
        "    enriched_df\n",
        "    .filter(col(\"weather_avg_tmpr_c\").isNotNull())\n",
        "    .groupBy(\"country\")\n",
        "    .agg(\n",
        "        count(\"*\").alias(\"record_count\"),\n",
        "        avg(\"weather_avg_tmpr_c\").alias(\"avg_temperature_c\")\n",
        "    )\n",
        "    .orderBy(col(\"record_count\").desc())\n",
        "    .show()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Idempotency Test\n",
        "\n",
        "Verify that running the ETL multiple times produces the same result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing idempotency...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First run count:  1997\n",
            "Second run count: 1997\n",
            "\n",
            "✓ Idempotency verified: True\n"
          ]
        }
      ],
      "source": [
        "# Test idempotency - run the write again\n",
        "print(\"Testing idempotency...\")\n",
        "\n",
        "# First run already done, read result\n",
        "result1 = spark.read.parquet(OUTPUT_PATH)\n",
        "count1 = result1.count()\n",
        "\n",
        "# Second run\n",
        "enriched_df.write.mode(\"overwrite\").partitionBy(\"country\").parquet(OUTPUT_PATH)\n",
        "result2 = spark.read.parquet(OUTPUT_PATH)\n",
        "count2 = result2.count()\n",
        "\n",
        "print(f\"First run count:  {count1}\")\n",
        "print(f\"Second run count: {count2}\")\n",
        "print(f\"\\n✓ Idempotency verified: {count1 == count2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stop Spark session\n",
        "# Uncomment to stop the session when done\n",
        "# spark.stop()\n",
        "# print(\"Spark session stopped\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
